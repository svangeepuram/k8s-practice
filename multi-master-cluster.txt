multi-master cluster


178.128.72.232 : k8shost

adduser chary
usermod -aG sudo chary
root@k8shost:~# rsync --archive --chown=chary:chary ~/.ssh /home/chary

sudo apt install virtualbox virtualbox-ext-pack

wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add -
wget -q https://www.virtualbox.org/download/oracle_vbox.asc -O- | sudo apt-key add -

$(lsb_release -cs)

focal

echo "deb [arch=amd64] http://download.virtualbox.org/virtualbox/debian $(lsb_release -cs) contrib" | \
     sudo tee -a /etc/apt/sources.list.d/virtualbox.list

sudo apt update
sudo apt install virtualbox-6.1


not needed
wget https://download.virtualbox.org/virtualbox/6.1.8/Oracle_VM_VirtualBox_Extension_Pack-6.1.8.vbox-extpack
sudo VBoxManage extpack install Oracle_VM_VirtualBox_Extension_Pack-6.1.8.vbox-extpack

 ==
 install vagrant:

 curl -O https://releases.hashicorp.com/vagrant/2.2.9/vagrant_2.2.9_x86_64.deb
 sudo apt install ./vagrant_2.2.9_x86_64.deb

 vagrant --version
Vagrant 2.2.9
mkdir ~/my-vagrant-project
cd ~/my-vagrant-project

======
git clone https://github.com/mmumshad/kubernetes-the-hard-way.git

sudo apt-get autoremove virtualbox-dkms
sudo apt-get install build-essential linux-headers-`uname -r` dkms virtualbox-dkms

export  VAGRANT_DISABLE_VBOXSYMLINKCREATE=1
cd kubernetes-the-hard-way/vagrant
 change masters to 3

sudo vagrant up

sudo vagrant status

master-1                  running (virtualbox)
master-2                  running (virtualbox)
master-3                  running (virtualbox)
loadbalancer              running (virtualbox)
worker-1                  running (virtualbox)
worker-2                  running (virtualbox)

sudo cat <<EOF >> /etc/hosts
192.168.5.11  master-1
192.168.5.12  master-2
192.168.5.13  master-3
192.168.5.21  worker-1
192.168.5.22  worker-2
192.168.5.30  loadbalancer
EOF



ssh -i /home/chary/kubernetes-the-hard-way/vagrant/.vagrant/machines/master-1/virtualbox/private_key vagrant@master-1

sudo ssh -i /home/chary/kubernetes-the-hard-way/vagrant/.vagrant/machines/master-2/virtualbox/private_key vagrant@master-2

sudo ssh -i /home/chary/kubernetes-the-hard-way/vagrant/.vagrant/machines/master-3/virtualbox/private_key vagrant@master-3

sudo ssh -i /home/chary/kubernetes-the-hard-way/vagrant/.vagrant/machines/worker-1/virtualbox/private_key vagrant@worker-1

sudo ssh -i /home/chary/kubernetes-the-hard-way/vagrant/.vagrant/machines/worker-2/virtualbox/private_key vagrant@worker-2

sudo ssh -i /home/chary/kubernetes-the-hard-way/vagrant/.vagrant/machines/loadbalancer/virtualbox/private_key vagrant@loadbalancer

master-1

ssh-keygen -t rsa -b 2048

cat .ssh/id_rsa.pub

cat <<EOF >> ~/.ssh/authorized_keys
ssh-rsa public-key
EOF

on master :
install kubectl
$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)
v1.19.4

curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl
kubectl version --client

sudo sed -i '0,/RANDFILE/{s/RANDFILE/\#&/}' /etc/ssl/openssl.cnf

openssl genrsa -out ca.key 2048
openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr

openssl x509 -req -in ca.csr -signkey ca.key -CAcreateserial  -out ca.crt -days 1000

==
openssl genrsa -out admin.key 2048
openssl req -new -key admin.key -subj "/CN=admin/O=system:masters" -out admin.csr
openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out admin.crt -days 1000
==
openssl genrsa -out kube-controller-manager.key 2048
openssl req -new -key kube-controller-manager.key -subj "/CN=system:kube-controller-manager" -out kube-controller-manager.csr
openssl x509 -req -in kube-controller-manager.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out kube-controller-manager.crt -days 1000
==
openssl genrsa -out kube-proxy.key 2048
openssl req -new -key kube-proxy.key -subj "/CN=system:kube-proxy" -out kube-proxy.csr
openssl x509 -req -in kube-proxy.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out kube-proxy.crt -days 1000
==
openssl genrsa -out kube-scheduler.key 2048
openssl req -new -key kube-scheduler.key -subj "/CN=system:kube-scheduler" -out kube-scheduler.csr
openssl x509 -req -in kube-scheduler.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out kube-scheduler.crt -days 1000
==

cat > openssl.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 10.96.0.1
IP.2 = 192.168.5.11
IP.3 = 192.168.5.12
IP.4 = 192.168.5.13
IP.5 = 192.168.5.30
IP.6 = 127.0.0.1
EOF

==
openssl genrsa -out kube-apiserver.key 2048
openssl req -new -key kube-apiserver.key -subj "/CN=kube-apiserver" -out kube-apiserver.csr -config openssl.cnf
openssl x509 -req -in kube-apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out kube-apiserver.crt -extensions v3_req -extfile openssl.cnf -days 1000

==

cat > openssl-etcd.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
IP.1 = 192.168.5.11
IP.2 = 192.168.5.12
IP.3 = 192.168.5.13
IP.4 = 127.0.0.1
EOF

==

openssl genrsa -out etcd-server.key 2048
openssl req -new -key etcd-server.key -subj "/CN=etcd-server" -out etcd-server.csr -config openssl-etcd.cnf
openssl x509 -req -in etcd-server.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out etcd-server.crt -extensions v3_req -extfile openssl-etcd.cnf -days 1000

==
openssl genrsa -out service-account.key 2048
openssl req -new -key service-account.key -subj "/CN=service-accounts" -out service-account.csr
openssl x509 -req -in service-account.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out service-account.crt -days 1000

==
-rw------- 1 vagrant vagrant  1675 Nov 30 06:46 ca.key
-rw-rw-r-- 1 vagrant vagrant   895 Nov 30 06:50 ca.csr
-rw-rw-r-- 1 vagrant vagrant  1001 Nov 30 06:50 ca.crt
-rw------- 1 vagrant vagrant  1679 Nov 30 06:51 admin.key
-rw-rw-r-- 1 vagrant vagrant   920 Nov 30 06:52 admin.csr
-rw-rw-r-- 1 vagrant vagrant  1025 Nov 30 06:52 admin.crt
-rw------- 1 vagrant vagrant  1675 Nov 30 06:53 kube-controller-manager.key
-rw-rw-r-- 1 vagrant vagrant   920 Nov 30 06:53 kube-controller-manager.csr
-rw-rw-r-- 1 vagrant vagrant  1025 Nov 30 06:53 kube-controller-manager.crt
-rw------- 1 vagrant vagrant  1675 Nov 30 06:53 kube-proxy.key
-rw-rw-r-- 1 vagrant vagrant   903 Nov 30 06:53 kube-proxy.csr
-rw-rw-r-- 1 vagrant vagrant  1009 Nov 30 06:53 kube-proxy.crt
-rw------- 1 vagrant vagrant  1679 Nov 30 06:53 kube-scheduler.key
-rw-rw-r-- 1 vagrant vagrant   907 Nov 30 06:53 kube-scheduler.csr
-rw-rw-r-- 1 vagrant vagrant  1013 Nov 30 06:53 kube-scheduler.crt
-rw-rw-r-- 1 vagrant vagrant   475 Nov 30 06:55 openssl.cnf
-rw------- 1 vagrant vagrant  1679 Nov 30 06:56 kube-apiserver.key
-rw-rw-r-- 1 vagrant vagrant  1155 Nov 30 06:56 kube-apiserver.csr
-rw-rw-r-- 1 vagrant vagrant  1245 Nov 30 06:56 kube-apiserver.crt
-rw-rw-r-- 1 vagrant vagrant   316 Nov 30 06:57 openssl-etcd.cnf
-rw------- 1 vagrant vagrant  1675 Nov 30 06:57 etcd-server.key
-rw-rw-r-- 1 vagrant vagrant   997 Nov 30 06:57 etcd-server.csr
-rw-rw-r-- 1 vagrant vagrant  1090 Nov 30 06:57 etcd-server.crt
-rw------- 1 vagrant vagrant  1675 Nov 30 06:58 service-account.key
-rw-rw-r-- 1 vagrant vagrant   899 Nov 30 06:58 service-account.csr
-rw-rw-r-- 1 vagrant vagrant    41 Nov 30 06:58 ca.srl
drwxr-xr-x 5 vagrant vagrant  4096 Nov 30 06:58 .
-rw-rw-r-- 1 vagrant vagrant  1005 Nov 30 06:58 service-account.crt
===

for instance in master-2 master-3; do
  scp ca.crt ca.key kube-apiserver.key kube-apiserver.crt \
    service-account.key service-account.crt \
    etcd-server.key etcd-server.crt \
    ${instance}:~/
done

====
export LOADBALANCER_ADDRESS=192.168.5.30

{
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://${LOADBALANCER_ADDRESS}:6443 \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config set-credentials system:kube-proxy \
    --client-certificate=kube-proxy.crt \
    --client-key=kube-proxy.key \
    --embed-certs=true \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-proxy \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
}

====
{
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=kube-controller-manager.crt \
    --client-key=kube-controller-manager.key \
    --embed-certs=true \
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-controller-manager \
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
}

==
{
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config set-credentials system:kube-scheduler \
    --client-certificate=kube-scheduler.crt \
    --client-key=kube-scheduler.key \
    --embed-certs=true \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-scheduler \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
}

===
{
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=admin.kubeconfig

  kubectl config set-credentials admin \
    --client-certificate=admin.crt \
    --client-key=admin.key \
    --embed-certs=true \
    --kubeconfig=admin.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=admin \
    --kubeconfig=admin.kubeconfig

  kubectl config use-context default --kubeconfig=admin.kubeconfig
}

====


for instance in worker-1 worker-2; do
  scp kube-proxy.kubeconfig ${instance}:~/
done

for instance in master-2 master-3; do
  scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${instance}:~/
done

==

export ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
cat > encryption-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
==
for instance in master-2 master-3; do
  scp encryption-config.yaml ${instance}:~/
done

===


Do it in master-1
and repeat on master-2 and master-3


wget -q --show-progress --https-only --timestamping \
  "https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz"

{
    tar -xvf etcd-v3.3.9-linux-amd64.tar.gz
    sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/
}

{
    sudo mkdir -p /etc/etcd /var/lib/etcd
    sudo cp ca.crt etcd-server.key etcd-server.crt /etc/etcd/
}

export INTERNAL_IP=$(ip addr show enp0s8 | grep "inet " | awk '{print $2}' | cut -d / -f 1)
echo $INTERNAL_IP

export ETCD_NAME=$(hostname -s)
echo $ETCD_NAME

cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/etcd-server.crt \\
  --key-file=/etc/etcd/etcd-server.key \\
  --peer-cert-file=/etc/etcd/etcd-server.crt \\
  --peer-key-file=/etc/etcd/etcd-server.key \\
  --trusted-ca-file=/etc/etcd/ca.crt \\
  --peer-trusted-ca-file=/etc/etcd/ca.crt \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${INTERNAL_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster master-1=https://192.168.5.11:2380,master-2=https://192.168.5.12:2380,master-3=https://192.168.5.13:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

{
  sudo systemctl daemon-reload
  sudo systemctl enable etcd
  sudo systemctl start etcd
  sudo systemctl status etcd
}

sudo journalctl -u etcd -f

ls /etc/etcd/etcd-server.crt /etc/etcd/etcd-server.key /etc/etcd/ca.crt
ssh master-2
===
ls -l
-rw-rw-r-- 1 vagrant vagrant  1001 Nov 30 07:02 ca.crt
-rw------- 1 vagrant vagrant  1675 Nov 30 07:02 ca.key
-rw------- 1 vagrant vagrant  1679 Nov 30 07:02 kube-apiserver.key
-rw-rw-r-- 1 vagrant vagrant  1245 Nov 30 07:02 kube-apiserver.crt
-rw------- 1 vagrant vagrant  1675 Nov 30 07:02 service-account.key
-rw-rw-r-- 1 vagrant vagrant  1005 Nov 30 07:02 service-account.crt
-rw------- 1 vagrant vagrant  1675 Nov 30 07:02 etcd-server.key
-rw-rw-r-- 1 vagrant vagrant  1090 Nov 30 07:02 etcd-server.crt
-rw------- 1 vagrant vagrant  5305 Nov 30 07:11 admin.kubeconfig
-rw------- 1 vagrant vagrant  5351 Nov 30 07:11 kube-controller-manager.kubeconfig
-rw------- 1 vagrant vagrant  5321 Nov 30 07:11 kube-scheduler.kubeconfig
drwxr-xr-x 5 vagrant vagrant  4096 Nov 30 07:14 .
-rw-rw-r-- 1 vagrant vagrant   240 Nov 30 07:14 encryption-config.yaml
=====

run commands and exit

===
ssh master-3
-rw------- 1 vagrant vagrant  5305 Nov 30 07:11 admin.kubeconfig
-rw-rw-r-- 1 vagrant vagrant  1001 Nov 30 07:02 ca.crt
-rw------- 1 vagrant vagrant  1675 Nov 30 07:02 ca.key
-rwxrwxr-x 1 vagrant vagrant 46016 Nov 30 05:49 cert_verify.sh
-rw-rw-r-- 1 vagrant vagrant   240 Nov 30 07:14 encryption-config.yaml
-rw-rw-r-- 1 vagrant vagrant  1090 Nov 30 07:02 etcd-server.crt
-rw------- 1 vagrant vagrant  1675 Nov 30 07:02 etcd-server.key
-rw-rw-r-- 1 vagrant vagrant  1245 Nov 30 07:02 kube-apiserver.crt
-rw------- 1 vagrant vagrant  1679 Nov 30 07:02 kube-apiserver.key
-rw------- 1 vagrant vagrant  5351 Nov 30 07:11 kube-controller-manager.kubeconfig
-rw------- 1 vagrant vagrant  5321 Nov 30 07:11 kube-scheduler.kubeconfig
-rw-rw-r-- 1 vagrant vagrant  1005 Nov 30 07:02 service-account.crt
-rw------- 1 vagrant vagrant  1675 Nov 30 07:02 service-account.key
====

bring up etcd service on each master :


Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service â†’ /etc/systemd/system/etcd.service.

===

testing:
sudo ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.crt \
  --cert=/etc/etcd/etcd-server.crt \
  --key=/etc/etcd/etcd-server.key

  sudo ETCDCTL_API=3 etcdctl --write-out=table endpoint status \
    --endpoints=https://127.0.0.1:2379 \
    --cacert=/etc/etcd/ca.crt \
    --cert=/etc/etcd/etcd-server.crt \
    --key=/etc/etcd/etcd-server.key

    sudo ETCDCTL_API=3 etcdctl --write-out=table endpoint health \
      --endpoints=https://127.0.0.1:2379 \
      --cacert=/etc/etcd/ca.crt \
      --cert=/etc/etcd/etcd-server.crt \
      --key=/etc/etcd/etcd-server.key

  sudo ETCDCTL_API=3 etcdctl member list \
  >   --endpoints=https://127.0.0.1:2379 \
  >   --cacert=/etc/etcd/ca.crt \
  >   --cert=/etc/etcd/etcd-server.crt \
  >   --key=/etc/etcd/etcd-server.key
  45bf9ccad8d8900a, started, master-2, https://192.168.5.12:2380, https://192.168.5.12:2379
  54a5796a6803f252, started, master-1, https://192.168.5.11:2380, https://192.168.5.11:2379
  da27c13c21936c01, started, master-3, https://192.168.5.13:2380, https://192.168.5.13:2379

  troubleshoot:
  sudo journalctl -u etcd -f

  =======================================
  Control plane :

  sudo mkdir -p /etc/kubernetes/config

  wget -q --show-progress --https-only --timestamping \
  "https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kube-apiserver" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kube-controller-manager" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kube-scheduler" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kubectl"

{
   chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
   sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/
}

 kubectl version

 ==
 configure api server:

{
  sudo mkdir -p /var/lib/kubernetes/

  sudo cp ca.crt ca.key kube-apiserver.crt kube-apiserver.key \
    service-account.key service-account.crt \
    etcd-server.key etcd-server.crt \
    encryption-config.yaml /var/lib/kubernetes/
}

export INTERNAL_IP=$(ip addr show enp0s8 | grep "inet " | awk '{print $2}' | cut -d / -f 1)
echo $INTERNAL_IP

cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${INTERNAL_IP} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.crt \\
  --enable-admission-plugins=NodeRestriction,ServiceAccount \\
  --enable-swagger-ui=true \\
  --enable-bootstrap-token-auth=true \\
  --etcd-cafile=/var/lib/kubernetes/ca.crt \\
  --etcd-certfile=/var/lib/kubernetes/etcd-server.crt \\
  --etcd-keyfile=/var/lib/kubernetes/etcd-server.key \\
  --etcd-servers=https://192.168.5.11:2379,https://192.168.5.12:2379,https://192.168.5.13:2379 \\
  --event-ttl=1h \\
  --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.crt \\
  --kubelet-client-certificate=/var/lib/kubernetes/kube-apiserver.crt \\
  --kubelet-client-key=/var/lib/kubernetes/kube-apiserver.key \\
  --kubelet-https=true \\
  --runtime-config=api/all=true \\
  --service-account-key-file=/var/lib/kubernetes/service-account.crt \\
  --service-cluster-ip-range=10.96.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kube-apiserver.crt \\
  --tls-private-key-file=/var/lib/kubernetes/kube-apiserver.key \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

 controller manager:
 sudo cp kube-controller-manager.kubeconfig /var/lib/kubernetes/


cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --address=0.0.0.0 \\
  --cluster-cidr=192.168.5.0/24 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.crt \\
  --cluster-signing-key-file=/var/lib/kubernetes/ca.key \\
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/var/lib/kubernetes/ca.crt \\
  --service-account-private-key-file=/var/lib/kubernetes/service-account.key \\
  --service-cluster-ip-range=10.96.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

==
scheduler:
sudo cp kube-scheduler.kubeconfig /var/lib/kubernetes/

cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --kubeconfig=/var/lib/kubernetes/kube-scheduler.kubeconfig \\
  --address=127.0.0.1 \\
  --leader-elect=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

==
start services :

{
  sudo systemctl daemon-reload
  sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
  sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler
  sleep 5
  sudo systemctl status kube-apiserver kube-controller-manager kube-scheduler
}

sudo journalctl -u kube-apiserver -f

=======

run in each master:

kubectl get componentstatuses --kubeconfig admin.kubeconfig
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health":"true"}
etcd-1               Healthy   {"health":"true"}
etcd-2               Healthy   {"health":"true"}

====

Login to load balancer :

install haproxy:

sudo apt-get update && sudo apt-get install -y haproxy

cat <<EOF | sudo tee /etc/haproxy/haproxy.cfg
frontend kubernetes
    bind 192.168.5.30:6443
    option tcplog
    mode tcp
    default_backend kubernetes-master-nodes

backend kubernetes-master-nodes
    mode tcp
    balance roundrobin
    option tcp-check
    server master-1 192.168.5.11:6443 check fall 3 rise 2
    server master-2 192.168.5.12:6443 check fall 3 rise 2
    server master-3 192.168.5.13:6443 check fall 3 rise 2
EOF


sudo service haproxy restart
sudo service haproxy status

==
[WARNING] 334/085357 (3551) : config : missing timeouts for frontend 'kubernetes'.
Nov 30 08:53:57 loadbalancer haproxy[3551]:    | While not properly invalid, you will certainly encounter various problems
Nov 30 08:53:57 loadbalancer haproxy[3551]:    | with such a configuration. To fix this, please ensure that all following
Nov 30 08:53:57 loadbalancer haproxy[3551]:    | timeouts are set to a non-zero value: 'client', 'connect', 'server'.
Nov 30 08:53:57 loadbalancer haproxy[3551]: [WARNING] 334/085357 (3551) : config : log format ignored for frontend 'kubernetes' since it has no log address.
Nov 30 08:53:57 loadbalancer haproxy[3551]: [WARNING] 334/085357 (3551) : config : missing timeouts for backend 'kubernetes-master-nodes'.
Nov 30 08:53:57 loadbalancer haproxy[3551]:    | While not properly invalid, you will certainly encounter various problems
Nov 30 08:53:57 loadbalancer haproxy[3551]:    | with such a configuration. To fix this, please ensure that all following
Nov 30 08:53:57 loadbalancer haproxy[3551]:    | timeouts are set to a non-zero value: 'client', 'connect', 'server'.
==

curl  https://192.168.5.30:6443/version -k


{
  "major": "1",
  "minor": "19",
  "gitVersion": "v1.19.4",
  "gitCommit": "d360454c9bcd1634cf4cc52d1867af5491dc9c5f",
  "gitTreeState": "clean",
  "buildDate": "2020-11-11T13:09:17Z",
  "goVersion": "go1.15.2",
  "compiler": "gc",
  "platform": "linux/amd64"
}

===============

on master, generate kubelet cert and pvt key

cat > openssl-worker-1.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
C = <country>
ST = <state>
L = <city>
O = <organization>
OU = <organization unit>
CN = <MASTER_IP>
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
DNS.1 = worker-1
IP.1 = 192.168.5.21
EOF

openssl genrsa -out worker-1.key 2048
openssl req -new -key worker-1.key -subj "/CN=system:node:worker-1/O=system:nodes" -out worker-1.csr -config openssl-worker-1.cnf
openssl x509 -req -in worker-1.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out worker-1.crt -extensions v3_req -extfile openssl-worker-1.cnf -days 1000

openssl-worker-1.cnf
-rw------- 1 vagrant vagrant     1675 Nov 30 09:00 worker-1.key
-rw-rw-r-- 1 vagrant vagrant     1029 Nov 30 09:00 worker-1.csr
-rw-rw-r-- 1 vagrant vagrant       41 Nov 30 09:01 ca.srl
drwxr-xr-x 7 vagrant vagrant     4096 Nov 30 09:01 .
-rw-rw-r-- 1 vagrant vagrant     1123 Nov 30 09:01 worker-1.crt
===
 on master 1:
kubelet config file:

export LOADBALANCER_ADDRESS=192.168.5.30

{
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://${LOADBALANCER_ADDRESS}:6443 \
    --kubeconfig=worker-1.kubeconfig

  kubectl config set-credentials system:node:worker-1 \
    --client-certificate=worker-1.crt \
    --client-key=worker-1.key \
    --embed-certs=true \
    --kubeconfig=worker-1.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:node:worker-1 \
    --kubeconfig=worker-1.kubeconfig

  kubectl config use-context default --kubeconfig=worker-1.kubeconfig
}

scp ca.crt worker-1.crt worker-1.key worker-1.kubeconfig worker-1:~/

===========================

on worker1:

On worker-1:

worker-1$ wget -q --show-progress --https-only --timestamping \
  https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kubelet

  sudo mkdir -p \
    /etc/cni/net.d \
    /opt/cni/bin \
    /var/lib/kubelet \
    /var/lib/kube-proxy \
    /var/lib/kubernetes \
    /var/run/kubernetes

{
    chmod +x kubectl kube-proxy kubelet
    sudo mv kubectl kube-proxy kubelet /usr/local/bin/
}

echo ${HOSTNAME}
{
  sudo mv ${HOSTNAME}.key ${HOSTNAME}.crt /var/lib/kubelet/
  sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
  sudo mv ca.crt /var/lib/kubernetes/
}

cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.crt"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.96.0.10"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
EOF

==
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --tls-cert-file=/var/lib/kubelet/${HOSTNAME}.crt \\
  --tls-private-key-file=/var/lib/kubelet/${HOSTNAME}.key \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

===
configiure kube proxy:

sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig

cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "192.168.5.0/24"
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

{
  sudo systemctl daemon-reload
  sudo systemctl enable kubelet kube-proxy
  sudo systemctl start kubelet kube-proxy
  sudo systemctl status kubelet kube-proxy
}

master-1$ kubectl get nodes --kubeconfig admin.kubeconfig

==========================
TLS bootstrapping:

on master 1 :
scp ca.crt worker-2:~/

Worker node 2 :


wget -q --show-progress --https-only --timestamping \
  https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kubelet

  sudo mkdir -p \
    /etc/cni/net.d \
    /opt/cni/bin \
    /var/lib/kubelet \
    /var/lib/kube-proxy \
    /var/lib/kubernetes \
    /var/run/kubernetes

{
    chmod +x kubectl kube-proxy kubelet
    sudo mv kubectl kube-proxy kubelet /usr/local/bin/
}
testing:
kubectl version


move ca cert:

sudo mv ca.crt /var/lib/kubernetes/

Create bootstrap token :
 on master-1 :

cat > bootstrap-token-07401b.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
  # Name MUST be of form "bootstrap-token-<token id>"
  name: bootstrap-token-07401b
  namespace: kube-system

# Type MUST be 'bootstrap.kubernetes.io/token'
type: bootstrap.kubernetes.io/token
stringData:
  # Human readable description. Optional.
  description: "The default bootstrap token generated by 'kubeadm init'."

  # Token ID and secret. Required.
  token-id: 07401b
  token-secret: f395accd246ae52d

  # Expiration. Optional.
  expiration: 2021-03-10T03:22:11Z

  # Allowed usages.
  usage-bootstrap-authentication: "true"
  usage-bootstrap-signing: "true"

  # Extra groups to authenticate the token as. Must start with "system:bootstrappers:"
  auth-extra-groups: system:bootstrappers:worker
EOF

kubectl create -f bootstrap-token-07401b.yaml
secret/bootstrap-token-07401b created

===
authorize kubelets to create CSRs

cat > csrs-for-bootstrapping.yaml <<EOF
# enable bootstrapping nodes to create CSR
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: create-csrs-for-bootstrapping
subjects:
- kind: Group
  name: system:bootstrappers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:node-bootstrapper
  apiGroup: rbac.authorization.k8s.io
EOF


kubectl create -f csrs-for-bootstrapping.yaml

clusterrolebinding.rbac.authorization.k8s.io/create-csrs-for-bootstrapping created


====
Authorize workers(kubelets) to approve CSR

cat > auto-approve-csrs-for-group.yaml <<EOF
# Approve all CSRs for the group "system:bootstrappers"
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: auto-approve-csrs-for-group
subjects:
- kind: Group
  name: system:bootstrappers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
  apiGroup: rbac.authorization.k8s.io
EOF


kubectl create -f auto-approve-csrs-for-group.yaml
clusterrolebinding.rbac.authorization.k8s.io/auto-approve-csrs-for-group created
======
Authorize workers(kubelets) to Auto Renew Certificates on expiration

cat > auto-approve-renewals-for-nodes.yaml <<EOF
# Approve renewal CSRs for the group "system:nodes"
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: auto-approve-renewals-for-nodes
subjects:
- kind: Group
  name: system:nodes
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
  apiGroup: rbac.authorization.k8s.io
EOF


kubectl create -f auto-approve-renewals-for-nodes.yaml

clusterrolebinding.rbac.authorization.k8s.io/auto-approve-renewals-for-nodes created

====
equivalent imperative commands:

Authorize workers(kubelets) to create CSR:
kubectl create clusterrolebinding create-csrs-for-bootstrapping --clusterrole=system:node-bootstrapper --group=system:bootstrappers

Authorize workers(kubelets) to approve CSR
kubectl create clusterrolebinding auto-approve-csrs-for-group --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers

Authorize workers(kubelets) to Auto Renew Certificates on expiration
kubectl create clusterrolebinding auto-approve-renewals-for-nodes --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes

=======

Configure Kubelet to TLS Bootstrap
It is now time to configure the second worker to TLS bootstrap using the token we
generated

For worker-1 we started by creating a kubeconfig file with the TLS certificates
that we manually generated.

Here, we don't have the certificates yet.
So we cannot create a kubeconfig file.
Instead we create a bootstrap-kubeconfig file with information
about the token we created.

===================

On worker 2:

sudo kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig set-cluster bootstrap --server='https://192.168.5.30:6443' --certificate-authority=/var/lib/kubernetes/ca.crt
sudo kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig set-credentials kubelet-bootstrap --token=07401b.f395accd246ae52d
sudo kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig set-context bootstrap --user=kubelet-bootstrap --cluster=bootstrap
sudo kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig use-context bootstrap

alternately:
cat <<EOF | sudo tee /var/lib/kubelet/bootstrap-kubeconfig
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /var/lib/kubernetes/ca.crt
    server: https://192.168.5.30:6443
  name: bootstrap
contexts:
- context:
    cluster: bootstrap
    user: kubelet-bootstrap
  name: bootstrap
current-context: bootstrap
kind: Config
preferences: {}
users:
- name: kubelet-bootstrap
  user:
    token: 07401b.f395accd246ae52d
EOF

testing:

sudo cat /var/lib/kubelet/bootstrap-kubeconfig
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /var/lib/kubernetes/ca.crt
    server: https://192.168.5.30:6443
  name: bootstrap
contexts:
- context:
    cluster: bootstrap
    user: kubelet-bootstrap
  name: bootstrap
current-context: bootstrap
kind: Config
preferences: {}
users:
- name: kubelet-bootstrap
  user:
    token: 07401b.f395accd246ae52d
=========

Create the kubelet-config.yaml configuration file:

cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.crt"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.96.0.10"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
EOF

Note: We are not specifying the certificate details - tlsCertFile and tlsPrivateKeyFile - in this file

=====
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --bootstrap-kubeconfig="/var/lib/kubelet/bootstrap-kubeconfig" \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --cert-dir=/var/lib/kubelet/pki/ \\
  --rotate-certificates=true \\
  --rotate-server-certificates=true \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

=================
kubernet proxy:
sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig

cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "192.168.5.0/24"
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
======

Start the Worker Services

{
  sudo systemctl daemon-reload
  sudo systemctl enable kubelet kube-proxy
  sudo systemctl start kubelet kube-proxy
  sudo systemctl status kubelet kube-proxy
}



=============
on master-1:
kubectl get csr

kubectl get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                 CONDITION
csr-qlg4b   53s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:07401b   Approved,Issued
csr-r9644   49s   kubernetes.io/kubelet-serving                 system:node:worker-2      Pending

myhost="worker-2"
mycsr=$(kubectl get csr | grep ${myhost} | cut -d' ' -f1)
echo $mycsr
kubectl certificate approve $mycsr
kubectl get csr $mycsr

kubectl certificate approve csr-r9644

approve and verify:
kubectl certificate approve csr-r9644
certificatesigningrequest.certificates.k8s.io/csr-r9644 approved
vagrant@master-1:~$ kubectl get csr
NAME        AGE     SIGNERNAME                                    REQUESTOR                 CONDITION
csr-qlg4b   2m16s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:07401b   Approved,Issued
csr-r9644   2m12s   kubernetes.io/kubelet-serving                 system:node:worker-2      Approved,Issued

=========
kubectl get nodes --kubeconfig admin.kubeconfig
NAME       STATUS     ROLES    AGE     VERSION
worker-1   NotReady   <none>   40m     v1.19.4
worker-2   NotReady   <none>   2m54s   v1.19.4
=========
Configuring kubectl for Remote Access
generate a kubeconfig file for the kubectl command line utility based on the admin user credentials.
 on master-1:
 
export KUBERNETES_LB_ADDRESS=192.168.5.30

{
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://${KUBERNETES_LB_ADDRESS}:6443

  kubectl config set-credentials admin \
    --client-certificate=admin.crt \
    --client-key=admin.key

  kubectl config set-context kubernetes-the-hard-way \
    --cluster=kubernetes-the-hard-way \
    --user=admin

  kubectl config use-context kubernetes-the-hard-way
}

====
on master-1 :
kubectl get componentstatuses

Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health":"true"}
etcd-1               Healthy   {"health":"true"}
etcd-2               Healthy   {"health":"true"}


kubectl get nodes
NAME       STATUS     ROLES    AGE    VERSION
worker-1   NotReady   <none>   44m    v1.19.4
worker-2   NotReady   <none>   7m5s   v1.19.4

====
deploy pod network :

worker-1 and worker-2 :
Install CNI plugins

wget https://github.com/containernetworking/plugins/releases/download/v0.7.5/cni-plugins-amd64-v0.7.5.tgz
sudo tar -xzvf cni-plugins-amd64-v0.7.5.tgz --directory /opt/cni/bin/

./
./flannel
./ptp
./host-local
./portmap
./tuning
./vlan
./host-device
./sample
./dhcp
./ipvlan
./macvlan
./loopback
./bridge

=======

Deploy  weave network  on the master:
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

serviceaccount/weave-net created
clusterrole.rbac.authorization.k8s.io/weave-net created
clusterrolebinding.rbac.authorization.k8s.io/weave-net created
role.rbac.authorization.k8s.io/weave-net created
rolebinding.rbac.authorization.k8s.io/weave-net created
daemonset.apps/weave-net created
==

verification:
kubectl get pods -n kube-system

==
kubectl get pods -n kube-system
NAME              READY   STATUS    RESTARTS   AGE
weave-net-bgv5n   2/2     Running   0          98s
weave-net-sf5dh   2/2     Running   0          99s
====

kube-apiserver-to-kubelet connectivity:

in this section you will configure RBAC permissions to allow the
Kubernetes API Server to access the Kubelet API on each worker node.
Access to the Kubelet API is required for retrieving metrics, logs,
and executing commands in pods.

This tutorial sets the Kubelet --authorization-mode flag to Webhook.
Webhook mode uses the SubjectAccessReview API to determine authorization.

=====

Create the system:kube-apiserver-to-kubelet ClusterRole with permissions to
access the Kubelet API and perform most common tasks associated with managing pods:

on master:
cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF

clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created

====

The Kubernetes API Server authenticates to the Kubelet as the kubernetes user
using the client certificate as defined by the --kubelet-client-certificate flag.

================
Cluster Role Blinding:

Bind the system:kube-apiserver-to-kubelet ClusterRole to the kubernetes user:

cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kube-apiserver
EOF

clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created

=====
DNS Add-on

deploy the DNS add-on which provides DNS based service discovery,
backed by CoreDNS, to applications running inside the Kubernetes cluster.


kubectl apply -f https://raw.githubusercontent.com/svangeepuram/k8s-practice/master/coredns.yaml
kubectl get pods -A
kubectl get pods -l k8s-app=kube-dns -n kube-system
kubectl get nodes
kubectl get pods -A
kubectl run busybox --image=busybox:1.28 --command -- sleep 3600
kubectl get pod busybox -o yaml
kubectl get pods -l run=busybox
kubectl exec -ti busybox -- nslookup kubernetes
kubectl create secret generic kubernetes-the-hard-way   --from-literal="mykey=mydata"
kubectl create deployment nginx --image=nginx
kubectl get pods -l app=nginx
kubectl get pods -A
kubectl expose deploy nginx --type=NodePort --port 80
PORT_NUMBER=$(kubectl get svc -l app=nginx -o jsonpath="{.items[0].spec.ports[0].nodePort}")
curl http://worker-1:$PORT_NUMBER
curl http://worker-2:$PORT_NUMBER
POD_NAME=$(kubectl get pods -l app=nginx -o jsonpath="{.items[0].metadata.name}")
kubectl logs $POD_NAME
kubectl exec -ti $POD_NAME -- nginx -v


=============================

etcd masters:
sudo apt update
sudo apt install etcd-client -y
 sudo ETCDCTL_API=3 etcdctl get   --endpoints=https://127.0.0.1:2379   --cacert=/etc/etcd/ca.crt   --cert=/etc/etcd/etcd-server.crt   --key=/etc/etcd/etcd-server.key  /registry/secrets/default/kubernetes-the-hard-way | hexdump -C

Read this:
https://blogs.infoblox.com/community/coredns-for-kubernetes-service-discovery/

troubleshooting and recovery :

serviceaccount/coredns created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole
clusterrole.rbac.authorization.k8s.io/system:coredns created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
service/kube-dns created
error: unable to recognize "https://raw.githubusercontent.com/mmumshad/kubernetes-the-hard-way/master/deployments/coredns.yaml": no matches for kind "Deployment" in version "extensions/v1beta1"
vagrant@master-1:~$ kubectl get pods -l k8s-app=kube-dns -n kube-system
No resources found in kube-system namespace.


git clone https://github.com/coredns/deployment.git

cd deployment

===
going to helm :

install helm 3 on ubuntu

curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -
sudo apt-get install apt-transport-https --yes
echo "deb https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm

==clean above MESS

kubectl delete service kube-dns -n kube-system
kubectl delete configmap coredns -n kube-system
kubectl delete clusterrolebinding.rbac.authorization.k8s.io/system:coredns
kubectl delete clusterrole.rbac.authorization.k8s.io/system:coredns
kubectl delete serviceaccount coredns -n kube-system

===
log:
vagrant@master-1:~/deployment/kubernetes$ kubectl delete service kube-dns -n kube-system
service "kube-dns" deleted
vagrant@master-1:~/deployment/kubernetes$ kubectl delete configmap coredns -n kube-system
configmap "coredns" deleted
vagrant@master-1:~/deployment/kubernetes$ kubectl delete  clusterrolebinding clusterrolebinding.rbac.authorization.k8s.io/system:coredns -n kube-system
error: there is no need to specify a resource type as a separate argument when passing arguments in resource/name form (e.g. 'kubectl get resource/<resource_name>' instead of 'kubectl get resource resource/<resource_name>'
vagrant@master-1:~/deployment/kubernetes$ kubectl delete clusterrolebinding.rbac.authorization.k8s.io/system:coredns -n kube-system
warning: deleting cluster-scoped resources, not scoped to the provided namespace
clusterrolebinding.rbac.authorization.k8s.io "system:coredns" deleted
vagrant@master-1:~/deployment/kubernetes$ kubectl delete clusterrole.rbac.authorization.k8s.io/system:coredns
clusterrole.rbac.authorization.k8s.io "system:coredns" deleted
vagrant@master-1:~/deployment/kubernetes$ kubectl delete serviceaccount/coredns
Error from server (NotFound): serviceaccounts "coredns" not found
vagrant@master-1:~/deployment/kubernetes$ kubectl delete serviceaccount coredns
Error from server (NotFound): serviceaccounts "coredns" not found
vagrant@master-1:~/deployment/kubernetes$ kubectl delete serviceaccount coredns -n kube-system
serviceaccount "coredns" deleted
=====

helm repo add coredns https://raw.github.com/coredns/deployment/gh-pages/
helm install coredns/coredns --generate-name
===
helm install coredns/coredns --generate-name
NAME: coredns-1606733553
LAST DEPLOYED: Mon Nov 30 10:52:35 2020
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CoreDNS is now running in the cluster as a cluster-service.

It can be tested with the following:

1. Launch a Pod with DNS tools:

kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools

2. Query the DNS server:

/ # host kubernetes
=====


verification :
kubectl run --generator=run-pod/v1  busybox --image=busybox:1.28 --command -- sleep 3600







echo ${HOSTNAME}
{
  sudo mv ${HOSTNAME}.key ${HOSTNAME}.crt /var/lib/kubelet/
  sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
  sudo mv ca.crt /var/lib/kubernetes/
}

cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.crt"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.96.0.10"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
EOF

==
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --tls-cert-file=/var/lib/kubelet/${HOSTNAME}.crt \\
  --tls-private-key-file=/var/lib/kubelet/${HOSTNAME}.key \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

===
configiure kube proxy:

sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig

cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "192.168.5.0/24"
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

{
  sudo systemctl daemon-reload
  sudo systemctl enable kubelet kube-proxy
  sudo systemctl start kubelet kube-proxy
  sudo systemctl status kubelet kube-proxy
}

=====
smoke test:
kubectl create secret generic kubernetes-the-hard-way \
  --from-literal="mykey=mydata"

  sudo ETCDCTL_API=3 etcdctl get \
    --endpoints=https://127.0.0.1:2379 \
    --cacert=/etc/etcd/ca.crt \
    --cert=/etc/etcd/etcd-server.crt \
    --key=/etc/etcd/etcd-server.key\
    /registry/secrets/default/kubernetes-the-hard-way | hexdump -C
    Get:6 https://baltocdn.com/helm/stable/debian all/main amd64 Packages [1804 B]
    Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]
    Fetched 270 kB in 2s (119 kB/s)
    Reading package lists... Done
    vagrant@master-1:~/deployment/kubernetes$ sudo apt-get install helm
    Reading package lists... Done
    Building dependency tree
    Reading state information... Done
    The following NEW packages will be installed:
      helm
    0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
    Need to get 13.3 MB of archives.
    After this operation, 41.6 MB of additional disk space will be used.
    Get:1 https://baltocdn.com/helm/stable/debian all/main amd64 helm amd64 3.4.1-1 [13.3 MB]
    Fetched 13.3 MB in 1s (10.7 MB/s)
    Selecting previously unselected package helm.
    (Reading database ... 59841 files and directories currently installed.)
    Preparing to unpack .../helm_3.4.1-1_amd64.deb ...
    Unpacking helm (3.4.1-1) ...
    Setting up helm (3.4.1-1) ...
    Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
    vagrant@master-1:~/deployment/kubernetes$ kubectl delete service/kube-dns
    Error from server (NotFound): services "kube-dns" not found
    vagrant@master-1:~/deployment/kubernetes$ kubectl delete service kube-dns
    Error from server (NotFound): services "kube-dns" not found
    vagrant@master-1:~/deployment/kubernetes$ kubectl delete configmap/coredns
    Error from server (NotFound): configmaps "coredns" not found
    vagrant@master-1:~/deployment/kubernetes$ kubectl delete service kube-dns -n kube-system
    service "kube-dns" deleted
    vagrant@master-1:~/deployment/kubernetes$ kubectl delete configmap coredns -n kube-system
    configmap "coredns" deleted
    vagrant@master-1:~/deployment/kubernetes$ kubectl delete  clusterrolebinding clusterrolebinding.rbac.authorization.k8s.io/system:coredns -n kube-system
    error: there is no need to specify a resource type as a separate argument when passing arguments in resource/name form (e.g. 'kubectl get resource/<resource_name>' instead of 'kubectl get resource resource/<resource_name>'
    vagrant@master-1:~/deployment/kubernetes$ kubectl delete clusterrolebinding.rbac.authorization.k8s.io/system:coredns -n kube-system
    warning: deleting cluster-scoped resources, not scoped to the provided namespace
    clusterrolebinding.rbac.authorization.k8s.io "system:coredns" deleted
    vagrant@master-1:~/deployment/kubernetes$ kubectl delete clusterrole.rbac.authorization.k8s.io/system:coredns
    clusterrole.rbac.authorization.k8s.io "system:coredns" deleted
    vagrant@master-1:~/deployment/kubernetes$ kubectl delete serviceaccount/coredns
    Error from server (NotFound): serviceaccounts "coredns" not found
    vagrant@master-1:~/deployment/kubernetes$ kubectl delete serviceaccount coredns
    Error from server (NotFound): serviceaccounts "coredns" not found
    vagrant@master-1:~/deployment/kubernetes$ kubectl delete serviceaccount coredns -n kube-system
    serviceaccount "coredns" deleted
    vagrant@master-1:~/deployment/kubernetes$
    vagrant@master-1:~/deployment/kubernetes$
    vagrant@master-1:~/deployment/kubernetes$
    vagrant@master-1:~/deployment/kubernetes$ helm repo add coredns https://raw.github.com/coredns/deployment/gh-pages/
    "coredns" has been added to your repositories
    vagrant@master-1:~/deployment/kubernetes$ helm install coredns/coredns
    Error: must either provide a name or specify --generate-name
    vagrant@master-1:~/deployment/kubernetes$ clear

    vagrant@master-1:~/deployment/kubernetes$ helm install coredns/coredns --generate-name
    NAME: coredns-1606733553
    LAST DEPLOYED: Mon Nov 30 10:52:35 2020
    NAMESPACE: default
    STATUS: deployed
    REVISION: 1
    TEST SUITE: None
    NOTES:
    CoreDNS is now running in the cluster as a cluster-service.

    It can be tested with the following:

    1. Launch a Pod with DNS tools:

    kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools

    2. Query the DNS server:

    / # host kubernetes
    vagrant@master-1:~/deployment/kubernetes$ kubectl get pods -l k8s-app=kube-dns -n kube-system
    No resources found in kube-system namespace.
    vagrant@master-1:~/deployment/kubernetes$ kubectl get svc
    NAME                         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
    coredns-1606733553-coredns   ClusterIP   10.96.0.12   <none>        53/UDP,53/TCP   68s
    kubernetes                   ClusterIP   10.96.0.1    <none>        443/TCP         129m
    vagrant@master-1:~/deployment/kubernetes$ kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools
    If you don't see a command prompt, try pressing enter.
    dnstools# host kubernetes
    ;; connection timed out; no servers could be reached
    dnstools# nslookup kubernetes.cluster.local
    ;; connection timed out; no servers could be reached

    dnstools# exit
    pod "dnstools" deleted
    pod default/dnstools terminated (Error)
    vagrant@master-1:~/deployment/kubernetes$ kubectl run --generator=run-pod/v1  busybox --image=busybox:1.28 --command -- sleep 3600
    Flag --generator has been deprecated, has no effect and will be removed in the future.
    pod/busybox created
    vagrant@master-1:~/deployment/kubernetes$
    vagrant@master-1:~/deployment/kubernetes$ kubectl get pods
    NAME                                          READY   STATUS    RESTARTS   AGE
    busybox                                       1/1     Running   0          10s
    coredns-1606733553-coredns-6d8b99757b-9gh77   1/1     Running   0          6m18s
    vagrant@master-1:~/deployment/kubernetes$ kubectl describe pod coredns-1606733553-coredns-6d8b99757b-9gh77
    Name:         coredns-1606733553-coredns-6d8b99757b-9gh77
    Namespace:    default
    Priority:     0
    Node:         worker-1/192.168.5.21
    Start Time:   Mon, 30 Nov 2020 10:52:37 +0000
    Labels:       app.kubernetes.io/instance=coredns-1606733553
                  app.kubernetes.io/name=coredns
                  k8s-app=coredns
                  pod-template-hash=6d8b99757b
    Annotations:  checksum/config: 0b88d0accc504ae7cb6f47f3b0c04e01dbb6028391ed91202eb40f0c38734525
                  scheduler.alpha.kubernetes.io/critical-pod:
                  scheduler.alpha.kubernetes.io/tolerations: [{"key":"CriticalAddonsOnly", "operator":"Exists"}]
    Status:       Running
    IP:           10.32.0.2
    IPs:
      IP:           10.32.0.2
    Controlled By:  ReplicaSet/coredns-1606733553-coredns-6d8b99757b
    Containers:
      coredns:
        Container ID:  docker://54b47866dc89a8db64d0495251fc763a4884e44848394b5147cff29a205c444d
        Image:         coredns/coredns:1.8.0
        Image ID:      docker-pullable://coredns/coredns@sha256:cc8fb77bc2a0541949d1d9320a641b82fd392b0d3d8145469ca4709ae769980e
        Ports:         53/UDP, 53/TCP
        Host Ports:    0/UDP, 0/TCP
        Args:
          -conf
          /etc/coredns/Corefile
        State:          Running
          Started:      Mon, 30 Nov 2020 10:52:49 +0000
        Ready:          True
        Restart Count:  0
        Limits:
          cpu:     100m
          memory:  128Mi
        Requests:
          cpu:        100m
          memory:     128Mi
        Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
        Readiness:    http-get http://:8181/ready delay=10s timeout=5s period=10s #success=1 #failure=5
        Environment:  <none>
        Mounts:
          /etc/coredns from config-volume (rw)
          /var/run/secrets/kubernetes.io/serviceaccount from default-token-hv8z6 (ro)
    Conditions:
      Type              Status
      Initialized       True
      Ready             True
      ContainersReady   True
      PodScheduled      True
    Volumes:
      config-volume:
        Type:      ConfigMap (a volume populated by a ConfigMap)
        Name:      coredns-1606733553-coredns
        Optional:  false
      default-token-hv8z6:
        Type:        Secret (a volume populated by a Secret)
        SecretName:  default-token-hv8z6
        Optional:    false
    QoS Class:       Guaranteed
    Node-Selectors:  <none>
    Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                     node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
    Events:
      Type    Reason     Age    From               Message
      ----    ------     ----   ----               -------
      Normal  Scheduled  7m6s   default-scheduler  Successfully assigned default/coredns-1606733553-coredns-6d8b99757b-9gh77 to worker-1
      Normal  Pulling    7m1s   kubelet            Pulling image "coredns/coredns:1.8.0"
      Normal  Pulled     6m55s  kubelet            Successfully pulled image "coredns/coredns:1.8.0" in 5.616828492s
      Normal  Created    6m54s  kubelet            Created container coredns
      Normal  Started    6m54s  kubelet            Started container coredns
    vagrant@master-1:~/deployment/kubernetes$ kubectl get deploy
    NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
    coredns-1606733553-coredns   1/1     1            1           8m2s
    vagrant@master-1:~/deployment/kubernetes$ kubectl describe deploy coredns-1606733553-coredns
    Name:                   coredns-1606733553-coredns
    Namespace:              default
    CreationTimestamp:      Mon, 30 Nov 2020 10:52:37 +0000
    Labels:                 app.kubernetes.io/instance=coredns-1606733553
                            app.kubernetes.io/managed-by=Helm
                            app.kubernetes.io/name=coredns
                            helm.sh/chart=coredns-1.14.0
                            k8s-app=coredns
                            kubernetes.io/cluster-service=true
                            kubernetes.io/name=CoreDNS
    Annotations:            deployment.kubernetes.io/revision: 1
                            meta.helm.sh/release-name: coredns-1606733553
                            meta.helm.sh/release-namespace: default
    Selector:               app.kubernetes.io/instance=coredns-1606733553,app.kubernetes.io/name=coredns,k8s-app=coredns
    Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
    StrategyType:           RollingUpdate
    MinReadySeconds:        0
    RollingUpdateStrategy:  1 max unavailable, 25% max surge
    Pod Template:
      Labels:           app.kubernetes.io/instance=coredns-1606733553
                        app.kubernetes.io/name=coredns
                        k8s-app=coredns
      Annotations:      checksum/config: 0b88d0accc504ae7cb6f47f3b0c04e01dbb6028391ed91202eb40f0c38734525
                        scheduler.alpha.kubernetes.io/critical-pod:
                        scheduler.alpha.kubernetes.io/tolerations: [{"key":"CriticalAddonsOnly", "operator":"Exists"}]
      Service Account:  default
      Containers:
       coredns:
        Image:       coredns/coredns:1.8.0
        Ports:       53/UDP, 53/TCP
        Host Ports:  0/UDP, 0/TCP
        Args:
          -conf
          /etc/coredns/Corefile
        Limits:
          cpu:     100m
          memory:  128Mi
        Requests:
          cpu:        100m
          memory:     128Mi
        Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
        Readiness:    http-get http://:8181/ready delay=10s timeout=5s period=10s #success=1 #failure=5
        Environment:  <none>
        Mounts:
          /etc/coredns from config-volume (rw)
      Volumes:
       config-volume:
        Type:      ConfigMap (a volume populated by a ConfigMap)
        Name:      coredns-1606733553-coredns
        Optional:  false
    Conditions:
      Type           Status  Reason
      ----           ------  ------
      Available      True    MinimumReplicasAvailable
      Progressing    True    NewReplicaSetAvailable
    OldReplicaSets:  <none>
    NewReplicaSet:   coredns-1606733553-coredns-6d8b99757b (1/1 replicas created)
    Events:
      Type    Reason             Age    From                   Message
      ----    ------             ----   ----                   -------
      Normal  ScalingReplicaSet  8m23s  deployment-controller  Scaled up replica set coredns-1606733553-coredns-6d8b99757b to 1
    vagrant@master-1:~/deployment/kubernetes$ kubectl get cm -A
    NAMESPACE     NAME                                 DATA   AGE
    default       coredns-1606733553-coredns           1      9m4s
    kube-system   extension-apiserver-authentication   1      137m
    kube-system   weave-net                            0      56m
    vagrant@master-1:~/deployment/kubernetes$ kubectl get cm coredns-1606733553-coredns -o yaml
    apiVersion: v1
    data:
      Corefile: |-
        .:53 {
            errors
            health {
                lameduck 5s
            }
            ready
            kubernetes cluster.local in-addr.arpa ip6.arpa {
                pods insecure
                fallthrough in-addr.arpa ip6.arpa
                ttl 30
            }
            prometheus 0.0.0.0:9153
            forward . /etc/resolv.conf
            cache 30
            loop
            reload
            loadbalance
        }
    kind: ConfigMap
    metadata:
      annotations:
        meta.helm.sh/release-name: coredns-1606733553
        meta.helm.sh/release-namespace: default
      creationTimestamp: "2020-11-30T10:52:36Z"
      labels:
        app.kubernetes.io/instance: coredns-1606733553
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: coredns
        helm.sh/chart: coredns-1.14.0
        k8s-app: coredns
        kubernetes.io/cluster-service: "true"
        kubernetes.io/name: CoreDNS
      managedFields:
      - apiVersion: v1
        fieldsType: FieldsV1
        fieldsV1:
          f:data:
            .: {}
            f:Corefile: {}
          f:metadata:
            f:annotations:
              .: {}
              f:meta.helm.sh/release-name: {}
              f:meta.helm.sh/release-namespace: {}
            f:labels:
              .: {}
              f:app.kubernetes.io/instance: {}
              f:app.kubernetes.io/managed-by: {}
              f:app.kubernetes.io/name: {}
              f:helm.sh/chart: {}
              f:k8s-app: {}
              f:kubernetes.io/cluster-service: {}
              f:kubernetes.io/name: {}
        manager: Go-http-client
        operation: Update
        time: "2020-11-30T10:52:36Z"
      name: coredns-1606733553-coredns
      namespace: default
      resourceVersion: "18087"
      selfLink: /api/v1/namespaces/default/configmaps/coredns-1606733553-coredns
      uid: 64829de0-4f6a-4c68-8cb8-d8448e6f4917
    vagrant@master-1:~/deployment/kubernetes$ kubectl get ep
    NAME                         ENDPOINTS                                               AGE
    coredns-1606733553-coredns   10.32.0.2:53,10.32.0.2:53                               9m44s
    kubernetes                   192.168.5.11:6443,192.168.5.12:6443,192.168.5.13:6443   137m
    vagrant@master-1:~/deployment/kubernetes$ kubectl get svc
    NAME                         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
    coredns-1606733553-coredns   ClusterIP   10.96.0.12   <none>        53/UDP,53/TCP   9m54s
    kubernetes                   ClusterIP   10.96.0.1    <none>        443/TCP         138m
    vagrant@master-1:~/deployment/kubernetes$ ping 10.96.0.10
    PING 10.96.0.10 (10.96.0.10) 56(84) bytes of data.
    ^C
    --- 10.96.0.10 ping statistics ---
    5 packets transmitted, 0 received, 100% packet loss, time 4090ms

    vagrant@master-1:~/deployment/kubernetes$ ping 10.96.0.12
    PING 10.96.0.12 (10.96.0.12) 56(84) bytes of data.
    ^C
    --- 10.96.0.12 ping statistics ---
    6 packets transmitted, 0 received, 100% packet loss, time 5102ms

    vagrant@master-1:~/deployment/kubernetes$
    vagrant@master-1:~/deployment/kubernetes$ cd
    vagrant@master-1:~$ kubectl create secret generic kubernetes-the-hard-way \
    >   --from-literal="mykey=mydata"
    secret/kubernetes-the-hard-way created
    vagrant@master-1:~$ sudo ETCDCTL_API=3 etcdctl get \
    >   --endpoints=https://127.0.0.1:2379 \
    >   --cacert=/etc/etcd/ca.crt \
    >   --cert=/etc/etcd/etcd-server.crt \
    >   --key=/etc/etcd/etcd-server.key\
    >   /registry/secrets/default/kubernetes-the-hard-way | hexdump -C
    00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
    00000010  73 2f 64 65 66 61 75 6c  74 2f 6b 75 62 65 72 6e  |s/default/kubern|
    00000020  65 74 65 73 2d 74 68 65  2d 68 61 72 64 2d 77 61  |etes-the-hard-wa|
    00000030  79 0a 6b 38 73 3a 65 6e  63 3a 61 65 73 63 62 63  |y.k8s:enc:aescbc|
    00000040  3a 76 31 3a 6b 65 79 31  3a 52 2c 67 d2 8d 3b 86  |:v1:key1:R,g..;.|
    00000050  8b 4d a4 fd f4 11 ce 24  82 77 a5 73 5d 77 b1 8e  |.M.....$.w.s]w..|
    00000060  17 be 81 5a c3 48 62 46  2a a3 a6 d7 91 e9 44 66  |...Z.HbF*.....Df|
    00000070  4c 2a fc 92 71 d9 82 14  b3 61 cf 63 25 62 d6 23  |L*..q....a.c%b.#|
    00000080  2c 44 c9 d9 67 99 1c 41  0f d4 2a 6b 7d a9 2b c8  |,D..g..A..*k}.+.|
    00000090  40 03 a9 ca 15 5a cf 08  f7 78 8b 04 3f 76 f8 ca  |@....Z...x..?v..|
    000000a0  3b 79 30 90 63 51 f9 d5  c1 e9 00 bc ac 4c 7a 01  |;y0.cQ.......Lz.|
    000000b0  c9 7b 02 02 e2 47 1d 85  ca 5d 5d 5d d7 de d0 91  |.{...G...]]]....|
    000000c0  61 8f a4 8e 63 12 ca a6  cb 3c 02 9b 0e ca 66 bc  |a...c....<....f.|
    000000d0  88 a8 54 15 01 00 66 aa  91 3c 24 2e b2 cd fe a1  |..T...f..<$.....|
    000000e0  79 00 30 62 a0 6e 37 9b  29 61 ac a3 89 e2 66 25  |y.0b.n7.)a....f%|
    000000f0  dc 4b cd 23 cf 6e 69 fc  bc d3 3a fc df 6e 2a 05  |.K.#.ni...:..n*.|
    00000100  02 3b ec b3 bf 94 52 5c  1e 90 63 6a b8 79 b8 82  |.;....R\..cj.y..|
    00000110  7c 6b 9c 8d 22 be 42 8c  f1 90 ab a1 7b be 6b 90  ||k..".B.....{.k.|
    00000120  e5 fe 12 f2 2e 82 c4 f7  3f 05 e4 0c 43 27 f7 88  |........?...C'..|
    00000130  a4 41 02 21 81 17 04 aa  bc 11 f3 5c 7b f2 04 6d  |.A.!.......\{..m|
    00000140  4a 8e 6a 49 16 51 4b b2  fe 6a 70 8e e0 a5 e9 0a  |J.jI.QK..jp.....|
    00000150  ce f5 98 bc 21 86 c6 01  59 0a                    |....!...Y.|
    0000015a
    vagrant@master-1:~$ kubectl delete secret kubernetes-the-hard-way
    secret "kubernetes-the-hard-way" deleted
    vagrant@master-1:~$ kubectl create deployment nginx --image=nginx
    deployment.apps/nginx created
    vagrant@master-1:~$ kubectl get pods -l app=nginx
    NAME                     READY   STATUS              RESTARTS   AGE
    nginx-6799fc88d8-2cn92   0/1     ContainerCreating   0          13s
    vagrant@master-1:~$ kubectl expose deploy nginx --type=NodePort --port 80
    service/nginx exposed
    vagrant@master-1:~$ PORT_NUMBER=$(kubectl get svc -l app=nginx -o jsonpath="{.items[0].spec.ports[0].nodePort}")
    vagrant@master-1:~$ echo $PORT_NUMBER
    30082
    ===
curl http://worker-1:$PORT_NUMBER
curl http://worker-2:$PORT_NUMBER

worker-1 did not work

POD_NAME=$(kubectl get pods -l app=nginx -o jsonpath="{.items[0].metadata.name}")
echo $POD_NAME
echo $POD_NAME
nginx-6799fc88d8-2cn92
vagrant@master-1:~$ kubectl logs $POD_NAME
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
10.32.0.1 - - [30/Nov/2020:11:11:21 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.58.0" "-"


kubectl exec -ti $POD_NAME -- nginx -v
nginx version: nginx/1.19.5

===

end to end test:

wget https://dl.google.com/go/go1.15.2.linux-amd64.tar.gz

sudo tar -C /usr/local -xzf go1.15.2.linux-amd64.tar.gz
export GOPATH="/home/vagrant/go"
export PATH=$PATH:/usr/local/go/bin:$GOPATH/bin

install kube test:

git clone https://github.com/kubernetes/test-infra.git

cd test-infra/
GO111MODULE=on go install ./kubetest

sudo apt install jq

K8S_VERSION=$(kubectl version -o json | jq -r '.serverVersion.gitVersion')
echo $K8S_VERSION
export KUBERNETES_CONFORMANCE_TEST=y
export KUBECONFIG="$HOME/.kube/config"

kubetest --provider=skeleton --test --test_args=â€--ginkgo.focus=\[Conformance\]â€ --extract ${K8S_VERSION} | tee test.out


===
kubectl create deployment mydep --image=nginx --replicas=5 --dry-run=client -o yaml | \
 kubectl set sa -f - serviceaccount1 --local --dry-run=client -o yaml
